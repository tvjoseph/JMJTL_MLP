{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JMJPFU\n",
    "13-Mar-2020\n",
    "\n",
    "Lord Bless this attempt of yours\n",
    "#### Experimenting with autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading https://files.pythonhosted.org/packages/5a/af/dabae171f885ec4d9b2fe6aaf74c1d50a8d32106d840b9fb8eb0095a578d/opencv_python-4.2.0.32-cp37-cp37m-win_amd64.whl (33.0MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\thomas.joseph\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from opencv-python) (1.16.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.2.0.32\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# and batch size\n",
    "EPOCHS = 20\n",
    "INIT_LR = 1e-3\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "# load the MNIST dataset\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build our unsupervised dataset of images with a small amount of\n",
    "contamination (i.e., anomalies) added into it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating unsupervised dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] creating unsupervised dataset...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unsupervised_dataset(data, labels, validLabel=1,\n",
    "\tanomalyLabel=3, contam=0.01, seed=42):\n",
    "\t# grab all indexes of the supplied class label that are *truly*\n",
    "\t# that particular label, then grab the indexes of the image\n",
    "\t# labels that will serve as our \"anomalies\"\n",
    "\tvalidIdxs = np.where(labels == validLabel)[0]\n",
    "\tanomalyIdxs = np.where(labels == anomalyLabel)[0]\n",
    "\t# randomly shuffle both sets of indexes\n",
    "\trandom.shuffle(validIdxs)\n",
    "\trandom.shuffle(anomalyIdxs)\n",
    "\t# compute the total number of anomaly data points to select\n",
    "\ti = int(len(validIdxs) * contam)\n",
    "\tanomalyIdxs = anomalyIdxs[:i]\n",
    "\t# use NumPy array indexing to extract both the valid images and\n",
    "\t# \"anomlay\" images\n",
    "\tvalidImages = data[validIdxs]\n",
    "\tanomalyImages = data[anomalyIdxs]\n",
    "\t# stack the valid images and anomaly images together to form a\n",
    "\t# single data matrix and then shuffle the rows\n",
    "\timages = np.vstack([validImages, anomalyImages])\n",
    "\tnp.random.seed(seed)\n",
    "\tnp.random.shuffle(images)\n",
    "\t# return the set of images\n",
    "\treturn images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = build_unsupervised_dataset(trainX, trainY, validLabel=1,anomalyLabel=3, contam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6809, 28, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6809, 28, 28, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a channel dimension to every image in the dataset, then scale\n",
    "# the pixel intensities to the range [0, 1]\n",
    "images = np.expand_dims(images, axis=-1)\n",
    "images = images.astype(\"float32\") / 255.0\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the training and testing split\n",
    "(trainX, testX) = train_test_split(images, test_size=0.2,\n",
    "\trandom_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1362, 28, 28, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct our convolutional autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n",
    "\t\t# initialize the input shape to be \"channels last\" along with\n",
    "\t\t# the channels dimension itself\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\t\t# define the input to the encoder\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = inputs\n",
    "\t\t# loop over the number of filters\n",
    "\t\tfor f in filters:\n",
    "\t\t\t# apply a CONV => RELU => BN operation\n",
    "\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\t# flatten the network and then construct our latent vector\n",
    "\t\tvolumeSize = K.int_shape(x)\n",
    "        #print('This is the volume size',volumeSize)\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tlatent = Dense(latentDim)(x)\n",
    "\t\t# build the encoder model\n",
    "\t\tencoder = Model(inputs, latent, name=\"encoder\")\n",
    "        # start building the decoder model which will accept the\n",
    "\t\t# output of the encoder as its inputs\n",
    "\t\tlatentInputs = Input(shape=(latentDim,))\n",
    "\t\tx = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
    "\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "\t\t# loop over our number of filters again, but this time in\n",
    "\t\t# reverse order\n",
    "\t\tfor f in filters[::-1]:\n",
    "\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,\n",
    "\t\t\t\tpadding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n",
    "\t\t# original depth of the image\n",
    "\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "\t\toutputs = Activation(\"sigmoid\")(x)\n",
    "\t\t# build the decoder model\n",
    "\t\tdecoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "\t\t# our autoencoder is the encoder + decoder\n",
    "\t\tautoencoder = Model(inputs, decoder(encoder(inputs)),\n",
    "\t\t\tname=\"autoencoder\")\n",
    "\t\t# return a 3-tuple of the encoder, decoder, and autoencoder\n",
    "\t\treturn (encoder, decoder, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 7, 7, 64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volumeSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building autoencoder...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] building autoencoder...\")\n",
    "(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                50192     \n",
      "=================================================================\n",
      "Total params: 69,392\n",
      "Trainable params: 69,200\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3136)              53312     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_4 (Ba (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_5 (Ba (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 109,377\n",
      "Trainable params: 109,185\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 16)                69392     \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 28, 28, 1)         109377    \n",
      "=================================================================\n",
      "Total params: 178,769\n",
      "Trainable params: 178,385\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5447 samples, validate on 1362 samples\n",
      "Epoch 1/20\n",
      "5447/5447 [==============================] - 21s 4ms/sample - loss: 0.0461 - val_loss: 0.0355\n",
      "Epoch 2/20\n",
      "5447/5447 [==============================] - 23s 4ms/sample - loss: 0.0132 - val_loss: 0.0064\n",
      "Epoch 3/20\n",
      "5447/5447 [==============================] - 24s 4ms/sample - loss: 0.0045 - val_loss: 0.0037\n",
      "Epoch 4/20\n",
      "5447/5447 [==============================] - 25s 5ms/sample - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 5/20\n",
      "5447/5447 [==============================] - 26s 5ms/sample - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 6/20\n",
      "5447/5447 [==============================] - 26s 5ms/sample - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 7/20\n",
      "5447/5447 [==============================] - 27s 5ms/sample - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 8/20\n",
      "5447/5447 [==============================] - 28s 5ms/sample - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 9/20\n",
      "5447/5447 [==============================] - 31s 6ms/sample - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 10/20\n",
      "5447/5447 [==============================] - 32s 6ms/sample - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 11/20\n",
      "5447/5447 [==============================] - 32s 6ms/sample - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 12/20\n",
      "5447/5447 [==============================] - 32s 6ms/sample - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 13/20\n",
      "5447/5447 [==============================] - 32s 6ms/sample - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 14/20\n",
      "5447/5447 [==============================] - 30s 5ms/sample - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 15/20\n",
      "5447/5447 [==============================] - 32s 6ms/sample - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 16/20\n",
      "5447/5447 [==============================] - 32s 6ms/sample - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 17/20\n",
      "5447/5447 [==============================] - 32s 6ms/sample - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 18/20\n",
      "5447/5447 [==============================] - 33s 6ms/sample - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 19/20\n",
      "5447/5447 [==============================] - 32s 6ms/sample - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 20/20\n",
      "5447/5447 [==============================] - 31s 6ms/sample - loss: 0.0016 - val_loss: 0.0021\n"
     ]
    }
   ],
   "source": [
    "# train the convolutional autoencoder\n",
    "H = autoencoder.fit(\n",
    "\ttrainX, trainX,\n",
    "\tvalidation_data=(testX, testX),\n",
    "\tepochs=EPOCHS,\n",
    "\tbatch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] making predictions...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] making predictions...\")\n",
    "decoded = autoencoder.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(decoded, gt, samples=10):\n",
    "\t# initialize our list of output images\n",
    "\toutputs = None\n",
    "\t# loop over our number of output samples\n",
    "\tfor i in range(0, samples):\n",
    "\t\t# grab the original image and reconstructed image\n",
    "\t\toriginal = (gt[i] * 255).astype(\"uint8\")\n",
    "\t\trecon = (decoded[i] * 255).astype(\"uint8\")\n",
    "\t\t# stack the original and reconstructed image side-by-side\n",
    "\t\toutput = np.hstack([original, recon])\n",
    "\t\t# if the outputs array is empty, initialize it as the current\n",
    "\t\t# side-by-side image display\n",
    "\t\tif outputs is None:\n",
    "\t\t\toutputs = output\n",
    "\t\t# otherwise, vertically stack the outputs\n",
    "\t\telse:\n",
    "\t\t\toutputs = np.vstack([outputs, output])\n",
    "\t# return the output images\n",
    "\treturn outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = visualize_predictions(decoded, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cv2.imwrite('recon_vis.png', vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving image data...\n"
     ]
    }
   ],
   "source": [
    "# serialize the image data to disk\n",
    "print(\"[INFO] saving image data...\")\n",
    "f = open('Output/images.pickle', \"wb\")\n",
    "f.write(pickle.dumps(images))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving autoencoder...\n"
     ]
    }
   ],
   "source": [
    "# serialize the autoencoder model to disk\n",
    "print(\"[INFO] saving autoencoder...\")\n",
    "#tf.contrib.saved_model.save_keras_model(autoencoder,'Output/autoencoder.model')\n",
    "autoencoder.save('Output/autoencoder.model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 16)                69392     \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 28, 28, 1)         109377    \n",
      "=================================================================\n",
      "Total params: 178,769\n",
      "Trainable params: 178,385\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "new_model = tf.keras.models.load_model('Output/autoencoder.model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the anomaly detector to detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6809, 28, 28, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = pickle.loads(open('Output/images.pickle', \"rb\").read())\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on our image data and initialize our list of\n",
    "# reconstruction errors\n",
    "decoded = autoencoder.predict(images)\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "# loop over all original images and their corresponding\n",
    "# reconstructions\n",
    "for (image, recon) in zip(images, decoded):\n",
    "\t# compute the mean squared error between the ground-truth image\n",
    "\t# and the reconstructed image, then add it to our list of errors\n",
    "\tmse = np.mean((image - recon) ** 2)\n",
    "\terrors.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] mse threshold: 0.009352713926210999\n",
      "[INFO] 69 outliers found\n"
     ]
    }
   ],
   "source": [
    "# compute the q-th quantile of the errors which serves as our\n",
    "# threshold to identify anomalies -- any data point that our model\n",
    "# reconstructed with > threshold error will be marked as an outlier\n",
    "thresh = np.quantile(errors, 0.98999)\n",
    "idxs = np.where(np.array(errors) >= thresh)[0]\n",
    "print(\"[INFO] mse threshold: {}\".format(thresh))\n",
    "print(\"[INFO] {} outliers found\".format(len(idxs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  61,  104,  322,  409,  576,  734, 1056, 1150, 1166, 1197, 1215,\n",
       "       1397, 1420, 1437, 1477, 1656, 1782, 1981, 1999, 2032, 2132, 2134,\n",
       "       2263, 2268, 2302, 2567, 2819, 2897, 3010, 3044, 3074, 3168, 3204,\n",
       "       3296, 3882, 3917, 3932, 4160, 4164, 4279, 4301, 4309, 4394, 4423,\n",
       "       4523, 4543, 4575, 4600, 4806, 4925, 5129, 5226, 5283, 5306, 5446,\n",
       "       5726, 5753, 5802, 5913, 5982, 6020, 6135, 6285, 6346, 6397, 6423,\n",
       "       6440, 6624, 6736], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the outputs array\n",
    "outputs = None\n",
    "# loop over the indexes of images with a high mean squared error term\n",
    "for i in idxs:\n",
    "\t# grab the original image and reconstructed image\n",
    "\toriginal = (images[i] * 255).astype(\"uint8\")\n",
    "\trecon = (decoded[i] * 255).astype(\"uint8\")\n",
    "\t# stack the original and reconstructed image side-by-side\n",
    "\toutput = np.hstack([original, recon])\n",
    "\t# if the outputs array is empty, initialize it as the current\n",
    "\t# side-by-side image display\n",
    "\tif outputs is None:\n",
    "\t\toutputs = output\n",
    "\t# otherwise, vertically stack the outputs\n",
    "\telse:\n",
    "\t\toutputs = np.vstack([outputs, output])\n",
    "# show the output visualization\n",
    "cv2.imshow(\"Output\", outputs)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Layer visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_4:0' shape=(None, 28, 28, 1) dtype=float32>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
